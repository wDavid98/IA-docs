{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"TP3znT8prVo6"},"source":["# <center> **CNN FUNDAMENTALS!** </center>\n","\n","*“It can be concluded that from now on, deep learning with CNN has to be considered as the primary candidate in essentially any visual recognition task.” [Razavian 2014]*\n","\n","##  **Outline**\n","1. [**Dealing with images and convolution**](#Imgs_conv)\n","2. [**CNN: Introduction of Convolutional neuronal networks**](#Intro_conv)\n","3. [**CNN: Visualizing activations**](#Act)\n","4. [**CNN: Initialization, dropout and other tricks**](#Tricks)\n","\n"]},{"cell_type":"code","metadata":{"id":"EZlEQPs6k4Fu"},"source":["#@title **Student:** Mount the Drive  (If you are in colab){ display-mode: \"form\" }\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/')\n","print(os.getcwd())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q6995yVmvSxc"},"source":["#@title **Load libraries**\n","#@markdown Here is important to change to GPU\n","\n","import sys\n","import numpy as np\n","import seaborn as sns\n","import tensorflow as tf\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fQDr0sctu5Be"},"source":["# **1. Dealing with images and convolution** <a name=\"Imgs_conv\"></a>\n","\n","Images are **matrices** of namely, three channels. Each **value** of the matrix is a **pixel** and  represents a color value w.r.t **intensities**, namely, between $[0-255]$ = $2^{8}$. In terms of color, each matrix represent the quantity of color at each channel.\n","\n","- **Objects in world are a set of pixels in images. Did you see the complexity of the problem?**\n","<img src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/dl_1.png?raw=true\" width=\"400\">\n","\n","\n","# **[CIFAR 10](https://https://www.cs.toronto.edu/~kriz/cifar.html)**\n","\n","The CIFAR (Canadian Institute For Advanced Research) - 10 is used for **small photo classification problem** on computer vision + machine learning (specially deep learning). The dataset is comprised of 60,000 images, where each one of them has a $(w,y)$ of 32×32 pixel color. Each image has photographs of objects from 10 classes.\n","\n","\n","```\n","[0: airplane, 1: automobile, 2: bird,\n","3: cat, 4: deer, 5: dog, 6: frog,\n","7: horse, 8: ship, 9: truck]\n","```\n","\n","Since we now know tha dataset classes, we should **go to the action!**\n","\n","1. Load CIFAR10\n"]},{"cell_type":"code","metadata":{"id":"dnnWeivwvBjD"},"source":["#@title **Coding:** loading the CIFAR10 dataset.\n","\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","y_train = y_train.reshape(y_train.shape[0])\n","y_test = y_test.reshape(y_test.shape[0])\n","\n","print(\"CIFAR 10\")\n","print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9nBeI_MvFwY"},"source":["#@title **Coding:** time to plot our classes\n","class_names = ['airplane','automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","plt.figure(figsize=(7,7))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(x_train[i], cmap=plt.cm.binary)\n","    plt.xlabel(class_names[y_train[i]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1yONtASvvIy"},"source":["#@title **Coding** now we have to organize the data.\n","\n","# Reshape the tensors\n","x_train = x_train.reshape([-1, 32, 32, 3])\n","x_test = x_test.reshape([-1, 32, 32, 3])\n","\n","# A must: having the matrixes on float\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","# Use one hot encodding vectors for targets\n","y_test = tf.keras.utils.to_categorical(y_test)\n","y_train = tf.keras.utils.to_categorical(y_train)\n","\n","# Normalizing the data\n","x_train /= 255\n","x_test /= 255\n","print('x_train shape:', x_train.shape)\n","print('Number of images in x_train', x_train.shape[0], \"y_train:\", y_train.shape)\n","print('Number of images in x_test', x_test.shape[0], \"y_test: \", y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8QxEFG2hv01-"},"source":["#**CHALLENGE**\n","\n","- Define a DNN architecture for CIFAR-10\n","- Use almost three layers\n","- Evaluate the performance of the approach\n"]},{"cell_type":"code","source":["#@title **Student solution** Build a DNN and summarize the architecture\n","\n","\"\"\"\n","Put your code here\n","\"\"\""],"metadata":{"id":"CC5qxGtgm4fF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title **Student solution** Compile and test the model\n","\n","\"\"\"\n","Put your code here\n","\"\"\""],"metadata":{"id":"626FkIYenjok"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wE09g3KM_Eu"},"source":["#@title **Coding:** Show the prediction for a specific image\n","\n","# Train the model\n","history = model.fit(x_train, y_train, epochs = 10, batch_size = 64, validation_data = (x_test, y_test))\n","\n","# Show the loss and accuracy of the model\n","fig = plt.figure(figsize = (10,5))\n","\n","#Accuracy\n","ax = fig.add_subplot(1, 2, 1)\n","ax.plot(history.history['accuracy'], label = 'Train Accuracy');\n","ax.plot(history.history['val_accuracy'], label = 'Validation Accuracy');\n","ax.set_xlabel('Epochs');\n","ax.set_ylabel('Accuracy');\n","ax.legend();\n","\n","#Loss\n","ax = fig.add_subplot(1, 2, 2)\n","ax.plot(history.history['loss'], label = 'train loss');\n","ax.plot(history.history['val_loss'], label = 'evaluation loss');\n","ax.legend();\n","ax.set_xlabel('Epochs');\n","ax.set_ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jDSDgUfR0dBK"},"source":["#@title **Coding:** What happened? Why the results are bad?\n","\n","%run code/dl_plot.py\n","index_vector =  128 #@param {type:\"integer\"}\n","\n","# Why do we use the predict and not the evaluate here?\n","predictions = model.predict(x_test)\n","\n","# Let's predict only one image\n","print(\"Predicted value:\",  np.argmax(predictions[index_vector]), \" Class: \", class_names[np.argmax(predictions[index_vector])])\n","print(\"Max prob: \", np.max(predictions[index_vector]), \"Ground truth: \", class_names[np.argmax(y_test ,axis=1)[index_vector]])\n","print(predictions.shape)\n","\n","# Plot the image\n","plt.figure(figsize=(6,3))\n","plt.subplot(1,2,1)\n","plot_image(index_vector, predictions, np.argmax(y_test ,axis=1), x_test)\n","plt.subplot(1,2,2)\n","plot_value_array(index_vector, predictions,  np.argmax(y_test ,axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V87aAWcKrVo8"},"source":["\n","#### **Dealing with image variations**\n","<img src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/dl_2.png?raw=true\" width=\"700\">\n","\n","Now, did you see the visual representation complexity? If not, you can take a [look here](https://gitlab.com/bivl2ab/academico/cursos-uis/ai/ai-2-uis-student/-/blob/master/complement_notes/VisualRepresentationComplexity.ipynb)\n","\n","- What type of problems do we have? Which are the limitations?\n","- Can we use prior knowledge from images? For instance, local properties?\n","\n","# **2. CNN: Introduction of Convolutional Neuronal Networks** <a name=\"Intro_conv\"></a>\n","\n","<img width=\"500px\" src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/CNN.png?raw=true\">  \n","\n","The Convolutional Neuronal Network (CNN) architectures are able to successfully capture the **Spatial dependencies** of the images, through the learning of **most descriptive filters**.\n","\n","### **Convolutions: the new neuron units to learn**\n","\n","The convolution is a **fundamental** operation to compute **visual features**.  For instance, a **KERNEL** mask with size $3 \\times 3$ is sliding around the image. At each position, each corresponding pixel is multiplied by corresponding value in **kernel**  and finally the values are sum up  to obtain a new estimated value.\n","\n","<img src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/Convolution_schematic.gif?raw=true\" style=\"width:200px\">\n","\n","For images (signals in 2D), the convolution operation could be defined as:\n","\n","$$I(x,y) = \\sum_{u=0}^{n} \\sum_{v=0}^{m} I(x,y)h(u-x, v-y)$$,\n","$$\\hat{I}(x,y) = I(x,y)*h(x,y)$$\n","\n","\n","In typical, images with three channels, the convolution works, as:\n","<img width=\"500\" src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/CNN-3.gif?raw=true\">\n","\n","Depending of **KERNEL**, there is produced different **features**, such as edges, color filters, smoothing, among much others. The problem es is then:\n","\n","- Which are the **BEST** kernels to represent any image classes ?\n","- The analytical answer is so complicated (may be impossible), so, we can **LEARN THE KERNELS**\n","- So, we can see the kernel as a **local neuron unit** and we can reformulte the problem to fix best weights  of convolution kernel!.\n","\n","From this approximation, we have new kind of deep learning architectures that nowadays represent the state-of-the-art in image representation and other related areas!...\n","\n","Some additional concepts to consider to reduce complexity:\n","- **Stride length**: The Kernel shifts every time to perform operation between Kernel and the portion of the image.\n","\n","- **Padding:** The process to add **artifitial** columns or rows to obtain same image input dimensions.\n","    - *Same padding:* add rows and colums to obtain in response the same image dimension\n","    - *Valid padding:* Not use any padding.\n","\n","\n","### **Main Convolutional (Conv) Layers**\n","\n","Then, if **neuron units** are **convolutions**, the **Layers** are basically a set of **learned kernels**, that hiearchically achieve a **very robust representations**. Nowadays, a CNN architecture is then form by these main layers:\n","\n","#### **Convolutional Layers**\n","\n","A set of convolutional kernels (filters) to turn input images into output images (responses). For instance, using **valid padding** and 8 filters, we obtain:\n","\n","<img width=\"200px\" src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/CNN-7.png?raw=true\">\n","\n","#### **Pooling Layers**\n","\n","These layers are speciallized on reduce spatial size of CNN features, allowing  decrease the computational time and  extracting **dominant features** which are rotational and positional invariant. Also allows as a Noise Suppressant\n","\n","There are two types of pooling: **Max-Pooling**, **Average pooling**.\n","\n","<img width=\"200px\" src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/CNN-4.jpeg?raw=true\">  \n","\n","<img width=\"400px\" src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/CNN-8.png?raw=true\">\n","\n","\n","\n","#### **Fully-Connected layer**\n","\n","At the end of CNN architectures, we namely use DNNs, that allows to learn non-linear combinations of the high-level features and robustly represent the descriptor for any input.\n","\n","<img width=\"400px\" src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/CNN-9.png?raw=true\">  \n","\n","\n","Now, we have the complete recipe:\n","\n","<img width=\"500px\" src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/CNN.png?raw=true\">  \n","\n","Since we learn all the theory **we can now go to the action!**\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"PedQsFgkrVpI"},"source":["#@title **Coding:** time to pay attention on the computational complexity\n","\n","model_CNN =  tf.keras.models.Sequential()\n","model_CNN.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n","model_CNN.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n","model_CNN.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","model_CNN.add(tf.keras.layers.Flatten())\n","model_CNN.add(tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_uniform'))\n","model_CNN.add(tf.keras.layers.Dense(10, activation='softmax'))\n","model_CNN.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F91qmQMCrVpL"},"source":["#@title **Coding** It's the same notation as DNN to compile model\n","opt = tf.keras.optimizers.SGD(lr=0.001, momentum=0.9)\n","model_CNN.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NML1xPXR03G"},"source":["#@title **Coding** Time to show prediction for a specific image\n","\n","# It's time to fit the model\n","history = model_CNN.fit(x_train, y_train, epochs=10,verbose=0, batch_size=64,validation_data=(x_test, y_test))\n","\n","#Time to plot the loss and accuracy\n","fig = plt.figure(figsize=(10,5))\n","\n","# Accuracy\n","ax = fig.add_subplot(1, 2, 1)\n","ax.plot(history.history['accuracy'], label='Train Accuracy');\n","ax.plot(history.history['val_accuracy'], label='Validation Accuracy');\n","ax.set_xlabel('Epochs');\n","ax.set_ylabel('Accuracy');\n","ax.legend();\n","\n","# Loss\n","ax = fig.add_subplot(1, 2, 2)\n","ax.plot(history.history['loss'], label='train loss');\n","ax.plot(history.history['val_loss'], label='evaluation loss');\n","ax.legend();\n","ax.set_xlabel('Epochs');\n","ax.set_ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qvT3ZYmoPURv"},"source":["#@title **Coding:**  Evaluate the results obtained\n","\n","%run code/dl_plot.py\n","index_vector =  427 #@param {type:\"integer\"}\n","\n","# Why do we use the predict and not the evaluate here?\n","predictions = model_CNN.predict(x_test)\n","\n","# Look at the predicted value for one sample\n","print (\"Predicted valie:\",  np.argmax(predictions[index_vector]), \" Class: \", class_names[np.argmax(predictions[index_vector])])\n","print (\"Max prob: \", np.max(predictions[index_vector]), \"Ground truth: \", class_names[np.argmax(y_test ,axis=1)[index_vector]])\n","\n","print(predictions.shape)\n","plt.figure(figsize=(6,3))\n","plt.subplot(1,2,1)\n","plot_image(index_vector, predictions, np.argmax(y_test ,axis=1), x_test)\n","plt.subplot(1,2,2)\n","plot_value_array(index_vector, predictions,  np.argmax(y_test ,axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9l-1CYcMOvId"},"source":["#**CHALLENGE**\n","\n","**Pay attention in which layer there is more parameters to learn.**\n","\n","Now, you can propose a new CNN architecture. Use additional convolution and pooling layers. You can also change:\n","\n","- The optimizer\n","- Epochs\n","- Batch size\n","- So may parameters to learn...\n","- Report yout results.\n"]},{"cell_type":"code","source":["#@title **Student solution** Build a CNN and train it\n","\n","# Sequential model\n","\"\"\"\n","Put your code here\n","\"\"\"\n","\n","# Compile\n","\"\"\"\n","Put your code here\n","\"\"\"\n","\n","# Fit the model\n","\"\"\"\n","Put your code here\n","\"\"\"\n","\n","# Showing the loss and accuracy\n","\"\"\"\n","Put your code here\n","\"\"\""],"metadata":{"id":"QCrRFPChQhv2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hlcZAYiFER-Q"},"source":["#**3. CNN: Visualizing activations**  <a name=\"Act\"></a>\n","\n","CNN architectures are not only important because their results, their efficiency, robust, hierarchical and descriptive visual models! In fact:\n","\n","- The  first ConvLayer captures the Low-Level features such as edges, color. Learn **BEST FEATURES** for a particular problem\n","- Then,  the next convlayer learn high-level features of these primitives\n","- From biology, is analogous to connectivity pattern of Neurons in the Human Brain. **Emulate visual cortex!**, only respond in a restricted region of visual field.\n","- The pre-processing required in a ConvNet is much lower as compared to other classification algorithms.\n","\n","- Performs a better fitting learning due to the reduction in the number of parameters.\n","  - For instace. For an image in DNN $(224 \\times 224 \\times 3) =105.528$ input features\n","  - And a hidden layer of 1024 (very small respect to the input) is $(105.528 \\times 1024)$ that gives **150 millons of parameters!**... imposible to train!\n","- Are invariant to spatial position of the objects. If an object is in different position of an image the DNN react very different.  \n","\n","#### We learn  features from low to high level !!\n","\n","<img width=\"700px\" src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/CNN-10.png?raw=true\">\n","\n","<img width=\"700px\" src=\"https://github.com/wDavid98/IA-docs/blob/main/data/cnn/DF_1.png?raw=true\">\n","\n","\n","Using **Tensorflow-Keras** we can visualize such activations, that represent the output of convolutions."]},{"cell_type":"code","metadata":{"id":"f_EJ56lfSaEX"},"source":["#@title **Coding**  How can we extract the layer outputs?\n","layer_outputs = [layer.output for layer in model_CNN.layers ]\n","layer_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRLTND-7Sx4n"},"source":["#@title **Coding** Visualize the first layer\n","activation_model = tf.keras.models.Model(inputs=model_CNN.input, outputs=layer_outputs)\n","array = np.expand_dims(x_test[427], axis=0)\n","activations = activation_model.predict(array)\n","plt.figure(figsize=(20,3))\n","for i in range(32):\n","    plt.subplot(2,16,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(activations[0][0,:, :, i], cmap='viridis')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4hWr4QGwTRLv"},"source":["#**CHALLENGE**\n","\n","Now, visualize second layer activations."]},{"cell_type":"code","source":["#@title **Student solution** visualize N layers\n","\"\"\"\n","Put you code here\n","\"\"\""],"metadata":{"id":"aejNQKv4ckIM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yWUNCgzHTxni"},"source":["#**4. CNN: Initialization, dropout and other tricks**\n","\n","There exist additional factors that could be implemented over CNN and in general over deep lerning approaches to achieve much better representations.\n","\n","\n","### **Initialization of weights to be learned**\n","\n","A dramatical parameter to properly converge or almost to obtain a faster covergence it is the initializaciont of parameters.  There exist many ways to initialize weights, that range from classical  distributions to more sophisticated methods. The most common are:\n","\n","- **Random normal** : uses a normal distribution for weights at each layer\n","- **Random uniform**: uses a unifrom distribution for weights at each layer\n","- **Truncated Normal** same than normal but values out of two standar deviation are discarded\n","\n","- **Lecun Uniform** it is like a uniform distribution but stablish a range `[-lim, lim]` with $lim = \\sqrt{(\\frac{3}{\\text{# input units}})}$\n","\n","\n","\n","- **Glorot normal** or Xavier initialization use a truncated normal version, but with a standard deviation defition such as:\n","\n","$$ sigma = \\sqrt{\\frac{2}{\\text{# input units} -  \\text{# output units}}}$$\n","\n","This glorot initialization it is used by default in keras  at each of the layers.\n","\n","**Lets go to try with some initialization**\n"]},{"cell_type":"code","metadata":{"id":"EWS2Z5JtDwMd"},"source":["#@title **Coding** Let's use a random initialization\n","\n","# Do the sequential model\n","model_in = tf.keras.models.Sequential()\n","model_in.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n","model_in.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","model_in.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","model_in.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","model_in.add(tf.keras.layers.Flatten())\n","model_in.add(tf.keras.layers.Dense(128, kernel_initializer='glorot_uniform', bias_initializer='he_uniform',  activation='relu'))\n","model_in.add(tf.keras.layers.Dense(10, activation='softmax'))\n","\n","# Time to compile\n","opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n","model_in.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Fit the model\n","history = model_in.fit(x_train, y_train, epochs=20,verbose=0, batch_size=32,validation_data=(x_test, y_test))\n","\n","# Plot the loss and accuracy\n","fig = plt.figure(figsize=(10,5))\n","ax = fig.add_subplot(1, 2, 1)\n","ax.plot(history.history['accuracy'], label='Train Accuracy');\n","ax.plot(history.history['val_accuracy'], label='Validation Accuracy');\n","ax.set_xlabel('Epochs');\n","ax.set_ylabel('Accuracy');\n","ax.legend();\n","\n","ax = fig.add_subplot(1, 2, 2)\n","ax.plot(history.history['loss'], label='train loss');\n","ax.plot(history.history['val_loss'], label='evaluation loss');\n","ax.legend();\n","ax.set_xlabel('Epochs');\n","ax.set_ylabel('Loss');\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nnU51lPqEcon"},"source":["#**CHALLENGE**\n","\n","1. What it is `he uniform `?\n","2. Test with other configuration to init layers and report results."]},{"cell_type":"code","metadata":{"id":"bw7gVFqsEoR9"},"source":["#@title **Student code**\n","\"\"\"\n","Put your code here\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ExjM1TVcEuzG"},"source":["### **Dropout to avoid overfitting**\n","\n","This technique randomly drop nodes out of the network with a consecuent regularizing effect, avoiding overfitting problems\n","\n","- some number of layer outputs are randomly ignored or “dropped out\"\n","- In Keras this effect is achieved by introducing a dropout layer\n","- the amount of nodes removed is specified as a parameter.\n","- In this example it is add Dropout layers after each max pooling layer\n","\n","**Lets go to try**"]},{"cell_type":"code","metadata":{"id":"s5JD3zV7FJLP"},"source":["#@title **Coding** CNN with dropout.\n","\n","# Do the sequential model\n","model_in_do = tf.keras.models.Sequential()\n","model_in_do.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n","model_in_do.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in_do.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","model_in_do.add(tf.keras.layers.Dropout(0.2))\n","model_in_do.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in_do.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in_do.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in_do.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in_do.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","model_in_do.add(tf.keras.layers.Dropout(0.2))\n","model_in_do.add(tf.keras.layers.Flatten())\n","model_in_do.add(tf.keras.layers.Dense(128, kernel_initializer='glorot_uniform', bias_initializer='he_uniform',  activation='relu'))\n","model_in_do.add(tf.keras.layers.Dropout(0.2))\n","model_in_do.add(tf.keras.layers.Dense(10, activation='softmax'))\n","\n","# Time to compile\n","opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n","model_in_do.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Fit the model\n","history = model_in_do.fit(x_train, y_train, epochs=20,verbose=0, batch_size=32,validation_data=(x_test, y_test))\n","\n","# Plot the loss and accuracy\n","fig = plt.figure(figsize=(10,5))\n","ax = fig.add_subplot(1, 2, 1)\n","ax.plot(history.history['accuracy'], label='Train Accuracy');\n","ax.plot(history.history['val_accuracy'], label='Validation Accuracy');\n","ax.set_xlabel('Epochs');\n","ax.set_ylabel('Accuracy');\n","ax.legend();\n","\n","ax = fig.add_subplot(1, 2, 2)\n","ax.plot(history.history['loss'], label='train loss');\n","ax.plot(history.history['val_loss'], label='evaluation loss');\n","ax.legend();\n","ax.set_xlabel('Epochs');\n","ax.set_ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U4hWbLi2FfGe"},"source":["### **Batch normalization**\n","\n","An additional process to normalize the activation outputs. This normalization acts as standar normalization by keeping mean activation close to **ZERO** and standar deviation close to **ONE**. This relative simple process allows to acelerate training network and reduce the overfitting [1].  \n","\n","**Lets go to try**"]},{"cell_type":"code","metadata":{"id":"GFge9ytkF3ml"},"source":["#@title **Coding** CNN with Batch normalization\n","\n","# Do the sequential model\n","model_in_do_ba = tf.keras.models.Sequential()\n","model_in_do_ba.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n","model_in_do_ba.add(tf.keras.layers.BatchNormalization())\n","model_in_do_ba.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in_do_ba.add(tf.keras.layers.BatchNormalization())\n","model_in_do_ba.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","model_in_do_ba.add(tf.keras.layers.Dropout(0.2))\n","model_in_do_ba.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in_do_ba.add(tf.keras.layers.BatchNormalization())\n","model_in_do_ba.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in_do_ba.add(tf.keras.layers.BatchNormalization())\n","model_in_do_ba.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in_do_ba.add(tf.keras.layers.BatchNormalization())\n","model_in_do_ba.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', bias_initializer='he_uniform', padding='same'))\n","model_in_do_ba.add(tf.keras.layers.BatchNormalization())\n","model_in_do_ba.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","model_in_do_ba.add(tf.keras.layers.Dropout(0.2))\n","model_in_do_ba.add(tf.keras.layers.Flatten())\n","model_in_do_ba.add(tf.keras.layers.Dense(128, kernel_initializer='glorot_uniform', bias_initializer='he_uniform',  activation='relu'))\n","model_in_do_ba.add(tf.keras.layers.Dropout(0.2))\n","model_in_do_ba.add(tf.keras.layers.Dense(10, activation='softmax'))\n","\n","# Time to compile\n","opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n","model_in_do_ba.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Fit the model\n","history = model_in_do_ba.fit(x_train, y_train, epochs=20,verbose=0, batch_size=32,validation_data=(x_test, y_test))\n","\n","# Plot the loss and accuracy\n","fig = plt.figure(figsize=(10,5))\n","ax = fig.add_subplot(1, 2, 1)\n","ax.plot(history.history['accuracy'], label='Train Accuracy');\n","ax.plot(history.history['val_accuracy'], label='Validation Accuracy');\n","ax.set_xlabel('Epochs');\n","ax.set_ylabel('Accuracy');\n","ax.legend();\n","\n","ax = fig.add_subplot(1, 2, 2)\n","ax.plot(history.history['loss'], label='train loss');\n","ax.plot(history.history['val_loss'], label='evaluation loss');\n","ax.legend();\n","ax.set_xlabel('Epochs');\n","ax.set_ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8YN4_lANKE1E"},"source":["#@title **Coding:** Time to evaluate the model\n","model_in_do_ba.evaluate(x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ouqolxamGxJ2"},"source":["#**CHALLENGE**\n","- Now try to use batch normalization without use dropout. **ONLY** use batch normalization and report the results\n","- Use batch normalization for some layers, justify which layer did you selected\n","- Finally, built a CNN that report your best results and visualize the predictions"]},{"cell_type":"code","metadata":{"id":"3cPA3GRUI__m"},"source":["#@title **Student code**\n","\n","# Sequential model\n","\"\"\"\n","Put your code here\n","\"\"\"\n","\n","# Compile\n","\"\"\"\n","Put your code here\n","\"\"\"\n","\n","# Fit the model\n","\"\"\"\n","Put your code here\n","\"\"\"\n","\n","# Showing the loss and accuracy\n","\"\"\"\n","Put your code here\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7KNdwefow7iO"},"source":["#@title **Coding** An overview of the results obtained\n","%run code/dl_plot.py\n","\n","predictions = model_in_do_ba.predict(x_test)\n","\n","num_rows = 5\n","num_cols = 3\n","num_images = num_rows*num_cols\n","plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n","for i in range(num_images):\n","    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n","    plot_image(i, predictions, np.argmax(y_test ,axis=1), x_test)\n","    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n","    plot_value_array(i, predictions, np.argmax(y_test ,axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2GSZQokdrVpW"},"source":["# References\n","\n","1. [Batch normalization](https://arxiv.org/pdf/1502.03167.pdf)\n","2. [Dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n","3. [A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285.pdf)\n","\n"]}]}